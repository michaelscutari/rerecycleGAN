/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train.py ...
Using 16bit Automatic Mixed Precision (AMP)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4
Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4
Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 4 processes
----------------------------------------------------------------------------------------------------

wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: michael-scutari (michael-scutari-duke-university). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.18.7
wandb: Run data is saved locally in ./wandb/run-20241208_120913-myfwgyge
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run tough-night-20
wandb: ‚≠êÔ∏è View project at https://wandb.ai/michael-scutari-duke-university/rerecycleGAN
wandb: üöÄ View run at https://wandb.ai/michael-scutari-duke-university/rerecycleGAN/runs/myfwgyge
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/users/mas296/projects/ece661-GAN-project/rerecycleGAN/checkpoints exists and is not empty.
LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]

   | Name             | Type                    | Params | Mode 
----------------------------------------------------------------------
0  | AtoB             | UNet                    | 12.1 M | train
1  | BtoA             | UNet                    | 12.1 M | train
2  | nextA            | ResNet                  | 6.7 M  | train
3  | nextB            | ResNet                  | 6.7 M  | train
4  | discriminatorA   | MultiScaleDiscriminator | 8.3 M  | train
5  | discriminatorB   | MultiScaleDiscriminator | 8.3 M  | train
6  | adversarial_loss | BCEWithLogitsLoss       | 0      | train
7  | cycle_loss       | L1Loss                  | 0      | train
8  | identity_loss    | L1Loss                  | 0      | train
9  | recycle_loss     | MSELoss                 | 0      | train
10 | recurrent_loss   | MSELoss                 | 0      | train
----------------------------------------------------------------------
54.2 M    Trainable params
0         Non-trainable params
54.2 M    Total params
216.721   Total estimated model params size (MB)
395       Modules in train mode
0         Modules in eval mode
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('lr_discriminator', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('lr_generator', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/users/mas296/micromamba/envs/ece661/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:240: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
slurmstepd: error: *** JOB 6820845 ON compsci-cluster-fitz-04 CANCELLED AT 2024-12-08T13:56:22 ***
